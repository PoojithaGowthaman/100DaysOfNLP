{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_posted</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2019-10-13 22:16:39+00:00</td>\n",
       "      <td>.....BY THE WAY, DON’T CALL ME AGAIN, I’LL CAL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2019-10-13 23:09:01+00:00</td>\n",
       "      <td>.@marklevinshow on @FoxNews is doing a big sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2019-10-13 23:27:49+00:00</td>\n",
       "      <td>The U.S. has the worst of the ISIS prisoners. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2019-10-14 00:10:14+00:00</td>\n",
       "      <td>Somebody please explain to Chris Wallace of Fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2019-10-14 00:34:01+00:00</td>\n",
       "      <td>“Serial killers get more Due Process than the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date_posted  \\\n",
       "71  2019-10-13 22:16:39+00:00   \n",
       "72  2019-10-13 23:09:01+00:00   \n",
       "73  2019-10-13 23:27:49+00:00   \n",
       "74  2019-10-14 00:10:14+00:00   \n",
       "75  2019-10-14 00:34:01+00:00   \n",
       "\n",
       "                                                tweet  \n",
       "71  .....BY THE WAY, DON’T CALL ME AGAIN, I’LL CAL...  \n",
       "72  .@marklevinshow on @FoxNews is doing a big sho...  \n",
       "73  The U.S. has the worst of the ISIS prisoners. ...  \n",
       "74  Somebody please explain to Chris Wallace of Fo...  \n",
       "75  “Serial killers get more Due Process than the ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file = r'C:\\Users\\groov\\PythonCode\\output\\trump_tweets\\trump_tweets.csv'\n",
    "\n",
    "df = pd.read_csv(csv_file, names=['date_posted', 'tweet'])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [“, There, are, no, felonies, ,, there, are, n...\n",
       "1    [So, many, people, conveniently, forget, that,...\n",
       "2    [..., ..good, health, ,, at, my, request, ,, P...\n",
       "3    [We, may, be, in, the, process, of, leaving, S...\n",
       "4    [..., .understands, that, while, we, only, had...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "df['tokens'] = df['tweet'].apply(tokenizer.tokenize)\n",
    "\n",
    "df['tokens'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "list(ngrams(df['tokens'][0:1], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('“', 'There', 'are'),\n",
       " ('There', 'are', 'no'),\n",
       " ('are', 'no', 'felonies'),\n",
       " ('no', 'felonies', ','),\n",
       " ('felonies', ',', 'there')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(df['tokens'][0], 3))[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we']\n",
      "['everywhere', 'himself', 'perhaps', 'those', 'formerly']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\groov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as stop_words_sklearn\n",
    "\n",
    "stop_words_nltk = nltk.corpus.stopwords.words('english')\n",
    "print(stop_words_nltk[0:5])\n",
    "print(list(stop_words_sklearn)[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384, 179, 318)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = []\n",
    "\n",
    "stop_words = stop_words + stop_words_nltk\n",
    "stop_words = stop_words + list(stop_words_sklearn)\n",
    "stop_words = list(set(stop_words))\n",
    "\n",
    "stop_words_mine = ['“', '”', ',', '@', '’', '!']\n",
    "stop_words = stop_words + stop_words_mine\n",
    "\n",
    "\n",
    "len(stop_words), len(stop_words_nltk), len(list(stop_words_sklearn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There felonies Impeachable offenses. The Constitution clear need bribery treason high crimes misdemeanors. You impeached conduct alleged case. AlanDersh Dershowitz. seanhannity A Scam'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [token for token in df['tokens'][0] if token not in stop_words]\n",
    "\" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming removes the small meaning differences of pluralization or posssessive endings of words to normalize vocabulary\n",
    "# it can reduce the precision score of your search results, but would improve the recall score for returning relevant docs\n",
    "# so it is often useful to be able to turn off stemming where you want precision\n",
    "\n",
    "# two of the most popular stemmers are Porter and Snowball. They were both created by the scientist Martin Porter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'there feloni impeach offenses. the constitut clear need briberi treason high crime misdemeanors. you impeach conduct alleg case. alandersh dershowitz. seanhann A scam'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "\" \".join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'there feloni impeach offenses. the constitut clear need briberi treason high crime misdemeanors. you impeach conduct alleg case. alandersh dershowitz. seanhann a scam'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "\" \".join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization can associate words together by their meaning even if their spelling is different\n",
    "# like stemming, this can make your model less precise but more general\n",
    "# it is potentially more accurate than stemming because it takes into account word meaning\n",
    "# some lemmatizers use the word's part of speech in addition to spelling to improve accuracy\n",
    "# so, lemmatizers are better than stemmers for most applications.\n",
    "\n",
    "# and if you really want the dimension reduction and recall improvement of a stemmer in your information \n",
    "# retrieval pipeline, you shoudl probably also use a lemmatizer right befor the stemmer.\n",
    "# because the lemma of a word is a valid English word, stemmers work well on the output of a lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\groov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'There felony Impeachable offenses. The Constitution clear need bribery treason high crime misdemeanors. You impeached conduct alleged case. AlanDersh Dershowitz. seanhannity A Scam'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "\" \".join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There felonies Impeachable offenses. The Constitution clear need bribery treason high crimes misdemeanors. You impeached conduct alleged case. AlanDersh Dershowitz. seanhannity A Scam'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(tokens) # slight difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when should you use a lemmatizer or a stemmer?\n",
    "# stemmers are generally faster to compute and require less-complex code and datasets. but stemmers make more errors\n",
    "# and stem a far greater number of words, reducing the meaning of your text much more than a lemmatizer will.\n",
    "\n",
    "# both stemmers will reduce the vocabulary of your text and increase the ambiguity of the text. but lemmatizers do\n",
    "# a better job at retaining information content of a word. so some packages such as spaCy do not provide stemming functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VADER was one of the first rule-based sentiment analysis algorithms. \n",
    "# it stands for Valence Aware Dictionary for sEntiment Reasoning.\n",
    "# nltk has an implementation of the VADER algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\groov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sa = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = []\n",
    "neu = []\n",
    "pos = []\n",
    "compound = []\n",
    "\n",
    "for tweet in df['tweet']:\n",
    "    print(tweet)\n",
    "    print(sa.polarity_scores(tweet))\n",
    "    print()\n",
    "    \n",
    "    neg.append(sa.polarity_scores(tweet)['neg'])\n",
    "    neu.append(sa.polarity_scores(tweet)['neu'])\n",
    "    pos.append(sa.polarity_scores(tweet)['pos'])\n",
    "    compound.append(sa.polarity_scores(tweet)['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_df = pd.DataFrame({'neg': neg, 'neu': neu, 'pos': pos, 'compound': compound, 'tweet': df['tweet']})\n",
    "sa_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "sa_df[['pos', 'neg']].plot.bar(stacked=True, figsize=(16,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.distplot(sa_df['neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(sa_df['pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in sa_df[sa_df['pos'] > 0.3]['tweet']:\n",
    "    print(tweet)\n",
    "    print(sa.polarity_scores(tweet))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in sa_df[sa_df['neg'] > 0.3]['tweet']:\n",
    "    print(tweet)\n",
    "    print(sa.polarity_scores(tweet))\n",
    "    print()\n",
    "    \n",
    "# the war one is interesting, because that's actually a very positive message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in sa_df[sa_df['neu'] > 0.9]['tweet']:\n",
    "    print(tweet)\n",
    "    print(sa.polarity_scores(tweet))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
